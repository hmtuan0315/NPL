{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68626933",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T11:08:21.858787Z",
     "start_time": "2021-06-15T11:08:21.831848Z"
    }
   },
   "outputs": [],
   "source": [
    "%config IPCompleter.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4bf490c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T11:08:27.882842Z",
     "start_time": "2021-06-15T11:08:21.862820Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "import spacy\n",
    "\n",
    "from torchtext.legacy.datasets import Multi30k\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "%run contractions.ipynb\n",
    "%run ../Utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e66d9ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T11:08:31.487840Z",
     "start_time": "2021-06-15T11:08:27.885846Z"
    }
   },
   "outputs": [],
   "source": [
    "spacy_de = spacy.load('de_core_news_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "242762a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T11:08:31.503838Z",
     "start_time": "2021-06-15T11:08:31.490829Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "SRC = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True, \n",
    "            batch_first = True)\n",
    "\n",
    "TRG = Field(tokenize = tokenize_de, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True, \n",
    "            batch_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "103316df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T11:08:39.833812Z",
     "start_time": "2021-06-15T11:08:31.506789Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.en', '.de'), \n",
    "                                                    fields = (SRC, TRG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b078d13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T11:08:39.849788Z",
     "start_time": "2021-06-15T11:08:39.836819Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.'], 'trg': ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.']}\n"
     ]
    }
   ],
   "source": [
    "for x in train_data:\n",
    "    print(vars(x))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a045276",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T11:08:40.539838Z",
     "start_time": "2021-06-15T11:08:39.852789Z"
    }
   },
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "435b34fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T11:08:40.555848Z",
     "start_time": "2021-06-15T11:08:40.544786Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source (de) vocabulary: 5893\n",
      "Unique tokens in target (en) vocabulary: 7853\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c62dfea2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T11:08:40.807821Z",
     "start_time": "2021-06-15T11:08:40.559789Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "     batch_size = BATCH_SIZE,\n",
    "     device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be42bc12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T11:08:43.393842Z",
     "start_time": "2021-06-15T11:08:40.810819Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 24])\n"
     ]
    }
   ],
   "source": [
    "for x in train_iterator:\n",
    "    print(x.src.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29372310",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T15:53:36.749405Z",
     "start_time": "2021-06-13T15:53:36.741421Z"
    }
   },
   "source": [
    "# Encoder\n",
    "<img src=\"./picture/encoder.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f0206c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-14T04:22:46.740432Z",
     "start_time": "2021-06-14T04:22:46.720444Z"
    }
   },
   "source": [
    "# Multi head attention\n",
    "## Overview\n",
    "\n",
    "<!-- <img src=\"./multi head attention.png\"> -->\n",
    "<img src=\"./picture/multi head attention 2.png\">\n",
    "\n",
    "**Encoder use self attention to encode the relationships between each word in an input sentence. As you can see in this pic below, The combination of Input Embedding and Position Embedding is fed to the Query, Key and Value of the first Encoder in the stack.**\n",
    "\n",
    "<img src=\"./picture/input into stack.png\">\n",
    "\n",
    "**The combination of Input Embedding and Position Embedding has the shape [batch_size, seq_len, embedding_size] (if you dont know why it has this shape, check out this [link](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html). So to make this simple, i just visualize a sample instead of a batch**\n",
    "\n",
    "<img src=\"./picture/sample.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12032133",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38b7ef47",
   "metadata": {},
   "source": [
    "**The first step is to calculate the Query, Key, and Value matrices. We do that by packing our embeddings into a matrix X, and multiplying it by the weight matrices we’ve trained (WQ, WK, WV)**\n",
    "\n",
    "<img src=\"./picture/weight.png\">\n",
    "\n",
    "**How do we get this weight? Well, We fetch the Query, Key, Value into the linear model, and each linear model has its own weight Wq, Wk, Wv. The input is passed through these Linear layers to product the Q, K, V matrices.(See this pic below for more detail)**\n",
    "\n",
    "<img src=\"./picture/weight2.png\">\n",
    "\n",
    "**Then we split data across Attention heads so that each can process it independently .We can achieve this by choosing the Query Size: \n",
    "Query Size = Embedding Size / Number of heads. Example when we choose head is 2:**\n",
    "\n",
    "<img src=\"./picture/heads.png\">\n",
    "\n",
    "**The first input has shape [batch_size, seq_len, embedding_size]. After splitting input into Attention heads, the shape becomes to [batch_size, seq_len, n_heads, query_size]. Now we reshape the Q, K, V matrices for compute Attention Score. We swap Head and Seq_len dimensions. This pic below visualizes one sample:**\n",
    "\n",
    "<img src=\"./picture/swap.png\">\n",
    "\n",
    "**We summerize all step for reshaping in this pic:**\n",
    "\n",
    "<img src=\"./picture/sumup.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0519bab",
   "metadata": {},
   "source": [
    "## Now we compute the Attention Score for each head\n",
    "\n",
    "<img src=\"./picture/score.png\">\n",
    "\n",
    "**The first step is to multiply Q and K matrix. (we must inverse one of these two matrix for matrix multiplication)** \n",
    "\n",
    "<img src=\"./picture/multi.png\">\n",
    "\n",
    "**Then a mask value is added to the result. In the Encoder Self-attention, the mask is used to mask out the Padding values so that they don’t participate in the Attention Score. The result of this step we call X**\n",
    "\n",
    "<img src=\"./picture/mask.png\">\n",
    "\n",
    "**Finally, we compute Attention score by multiplying softmax of X/sqrt(query_size) with V matrix**\n",
    "\n",
    "<img src=\"./picture/compute.png\">\n",
    "\n",
    "**We sum up all this step in this pic below:**\n",
    "\n",
    "<img src=\"./picture/summerize.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95377ce8",
   "metadata": {},
   "source": [
    "# Finnaly, we merge each Head’s Attention Scores together\n",
    "\n",
    "<img src=\"./picture/merge.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1016982",
   "metadata": {},
   "source": [
    "# End to end attention\n",
    "\n",
    "<img src=\"./picture/end-to-end-attention.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf158781",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T11:08:43.408794Z",
     "start_time": "2021-06-15T11:08:43.395788Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0225970b",
   "metadata": {},
   "source": [
    "### Before token and position embeddings are summed, The tokem embedding should be multiplied by a scaling factor which is sqrt(embedding_size). But why? I dont know. This aspect is not justified by the authors, either on the paper or anywhere else. It was specifically asked as an issue in the original implementation by Google with no response. [link](https://datascience.stackexchange.com/questions/87906/transformer-model-why-are-word-embeddings-scaled-before-adding-positional-encod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb8e6404",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T11:08:43.439793Z",
     "start_time": "2021-06-15T11:08:43.411789Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, pf_dim, n_layers, n_heads, max_length, p_drop):\n",
    "        super().__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.input_size = input_size\n",
    "        self.max_length = max_length\n",
    "        self.pf_dim = pf_dim\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        self.input_embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.positional_embedding = nn.Embedding(max_length, embedding_size)\n",
    "        self.encoder_layer = nn.ModuleList([EncoderLayer(embedding_size, pf_dim, n_heads, p_drop) for _ in range(n_layers)])\n",
    "        self.dropout = nn.Dropout(p_drop)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([embedding_size])).to(device)\n",
    "        \n",
    "    def forward(self, x, src_mask):\n",
    "        #x: [batch_size, seq_len]\n",
    "        # prepare input before fetch it into n layer \n",
    "        out = self.input_embedding(x)\n",
    "        #out:[batch_size, seq_len, embedding_size]\n",
    "        batch_size = x.shape[0]\n",
    "        src_len = x.shape[1]\n",
    "        #position embedding\n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(device)\n",
    "        #pos:[batch_size, src_len]\n",
    "        pos_embed = self.positional_embedding(pos)\n",
    "        #input\n",
    "        out = self.dropout(out * self.scale + pos_embed)\n",
    "        \n",
    "        for layer in self.encoder_layer:\n",
    "            out = layer(out, src_mask)\n",
    "        \n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf9e3d94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T11:08:43.470792Z",
     "start_time": "2021-06-15T11:08:43.442789Z"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.ModuleList):\n",
    "    def __init__(self, embedding_size, pf_size, n_heads, p_drop):\n",
    "        super().__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        self.multi_head_attention = MultiHeadAtt(embedding_size, n_heads, p_drop)\n",
    "        self.dropout = nn.Dropout(p_drop)\n",
    "        self.attention_norm = nn.LayerNorm(embedding_size)\n",
    "        self.feed_forward = FeedForward(embedding_size, pf_size, p_drop)\n",
    "        self.ff_norm = nn.LayerNorm(embedding_size)\n",
    "        self.pf_size = pf_size\n",
    "    \n",
    "    def forward(self,x, src_mask):\n",
    "        out1, _ = self.multi_head_attention(x, x, x , src_mask)\n",
    "        out = self.attention_norm(x + self.dropout(out1)) \n",
    "        out1 = self.feed_forward(out)\n",
    "        out = self.ff_norm(out + self.dropout(out1)) \n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15d157b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T11:08:43.501814Z",
     "start_time": "2021-06-15T11:08:43.473792Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadAtt(nn.Module):\n",
    "    def __init__(self, embedding_size, n_heads, p_drop):\n",
    "        super().__init__()\n",
    "        assert embedding_size % n_heads == 0\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = embedding_size // n_heads\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "        \n",
    "        self.fq = nn.Linear(embedding_size, embedding_size)\n",
    "        self.fk = nn.Linear(embedding_size, embedding_size)\n",
    "        self.fv = nn.Linear(embedding_size, embedding_size)\n",
    "        self.fo = nn.Linear(embedding_size, embedding_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p_drop)\n",
    "    \n",
    "    def forward(self, query, key, value, src_mask = None):\n",
    "        Q = self.fq(query)\n",
    "        K = self.fk(key)\n",
    "        V = self.fv(value)\n",
    "        batch_size = query.shape[0]\n",
    "        Q = Q.view(batch_size, self.n_heads, -1, self.head_dim)\n",
    "        K = K.view(batch_size, self.n_heads, -1, self.head_dim)\n",
    "        V = V.view(batch_size, self.n_heads, -1, self.head_dim)\n",
    "        \n",
    "        QK = torch.matmul(Q, K.permute(0,1,3,2)) / self.scale\n",
    "        if src_mask is not None:\n",
    "            QK = QK.masked_fill(src_mask == 0, -1e10)\n",
    "        #softmax for last dimension\n",
    "        attention = torch.softmax(QK, dim = -1)\n",
    "        \n",
    "        out = torch.matmul(self.dropout(attention), V)\n",
    "        out = out.permute(0, 2, 1, 3).contiguous()\n",
    "        out = out.view(batch_size, -1, self.embedding_size)\n",
    "        out = self.fo(out)\n",
    "        return out, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be131608",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T11:08:43.517818Z",
     "start_time": "2021-06-15T11:08:43.504802Z"
    }
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embedding_dim, pf_dim, p_drop):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.pf_dim = pf_dim\n",
    "        self.fc1 = nn.Linear(embedding_dim, pf_dim)\n",
    "        self.fc2 = nn.Linear(pf_dim, embedding_dim)\n",
    "        self.dropout = nn.Dropout(p_drop)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #x = [batch_size, seq_len, embedding_dim]\n",
    "        out = self.dropout(torch.relu(self.fc1(x)))\n",
    "        #x = [batch size, seq len, pf dim]\n",
    "        out = self.fc2(out)\n",
    "        #x = [batch size, seq len, embedding dim]\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87476eb8",
   "metadata": {},
   "source": [
    "# Decoder\n",
    "\n",
    "<img src=\"./picture/decoder.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98482ce5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T11:08:43.549786Z",
     "start_time": "2021-06-15T11:08:43.519792Z"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,output_size, embedding_dim, pf_dim, n_layers, n_heads ,max_length, p_drop):\n",
    "        super().__init__()\n",
    "        self.output_size = output_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.pf_dim = pf_dim\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.input_embedding = nn.Embedding(output_size, embedding_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, embedding_dim)\n",
    "        self.dropout = nn.Dropout(p_drop)\n",
    "        self.decoder_layer = nn.ModuleList([DecoderLayer(embedding_dim, pf_dim, n_heads, p_drop) for _ in range(n_layers)])\n",
    "        self.fc = nn.Linear(embedding_dim, output_size)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([embedding_dim])).to(device)\n",
    "        \n",
    "    def forward(self, x, enc_src, trg_mask, src_mask):\n",
    "        out = self.input_embedding(x)\n",
    "        batch_size = x.shape[0]\n",
    "        src_len = x.shape[1]\n",
    "        #position embedding\n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(device)\n",
    "        pos = self.pos_embedding(pos)\n",
    "        \n",
    "        out = self.dropout(out * self.scale + pos)\n",
    "        for layer in self.decoder_layer:\n",
    "            out, attention = layer(out, enc_src, trg_mask, src_mask)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, attention\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcea2cb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T11:08:43.580790Z",
     "start_time": "2021-06-15T11:08:43.552820Z"
    }
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, embedding_size, pf_dim, n_heads, p_drop):\n",
    "        super().__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.pf_dim = pf_dim\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        self.self_attention = MultiHeadAtt(embedding_size, n_heads, p_drop)\n",
    "        self.encoder_attention = MultiHeadAtt(embedding_size, n_heads, p_drop)\n",
    "        self.attention_norm = nn.LayerNorm(embedding_size)\n",
    "        self.ff_norm = nn.LayerNorm(embedding_size)\n",
    "        self.encode_norm = nn.LayerNorm(embedding_size)\n",
    "        self.feed_forward = FeedForward(embedding_size,pf_dim, p_drop)\n",
    "        self.dropout = nn.Dropout(p_drop)\n",
    "    \n",
    "    def forward(self, x, enc_output, trg_mask, src_mask):\n",
    "        out1, _ = self.self_attention(x, x, x, trg_mask)\n",
    "        out = self.attention_norm(x + self.dropout(out1))\n",
    "        out1, attention = self.encoder_attention(out, enc_output, enc_output, src_mask)\n",
    "        out = self.encode_norm(out + self.dropout(out1))\n",
    "        out1 = self.feed_forward(out)\n",
    "        out = self.ff_norm(out + self.dropout(out1))\n",
    "        \n",
    "        return out, attention\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3955ec",
   "metadata": {},
   "source": [
    "# For Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "964e9f31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T11:08:43.595822Z",
     "start_time": "2021-06-15T11:08:43.583821Z"
    }
   },
   "outputs": [],
   "source": [
    "# INPUT_SIZE = 15\n",
    "# EMBEDDING_SIZE = 120\n",
    "# PF_SIZE = 256\n",
    "# N_LAYERS = 3\n",
    "# N_HEADS = 5\n",
    "# # encoder = Encoder(INPUT_SIZE, EMBEDDING_SIZE, PF_SIZE, N_HEADS, N_LAYERS).to(device)\n",
    "# decoder = Decoder(OUTPUT_SIZE, EMBEDDING_SIZE, PF_SIZE, DEC_LAYERS, N_HEADS, MAX_LENGTH, DEC_DROPOUT).to(device)\n",
    "\n",
    "# input = torch.randint(2, 5, (50, 10)).to(device)\n",
    "# src_mask = (input != 1).unsqueeze(1).unsqueeze(2)\n",
    "# # res_en = encoder(input, src_mask)\n",
    "# res = decoder(input, res_en, 1, 1)\n",
    "# res.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b676170",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T11:08:43.611786Z",
     "start_time": "2021-06-15T11:08:43.598821Z"
    }
   },
   "outputs": [],
   "source": [
    "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3bf45697",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T11:08:43.626845Z",
     "start_time": "2021-06-15T11:08:43.614789Z"
    }
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def create_src_mask(self, src):\n",
    "        #src = [batch size, src len]\n",
    "        src_mask = (src != SRC_PAD_IDX).unsqueeze(1).unsqueeze(2)\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        return src_mask\n",
    "    \n",
    "    def create_trg_mask(self, trg):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        \n",
    "        trg_pad_mask = (trg != TRG_PAD_IDX).unsqueeze(1).unsqueeze(2)\n",
    "        \n",
    "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
    "        \n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = device)).bool()\n",
    "        \n",
    "        #trg_sub_mask = [trg len, trg len]\n",
    "            \n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        \n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        return trg_mask\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #trg = [batch size, trg len]\n",
    "                \n",
    "        src_mask = self.create_src_mask(src)\n",
    "        trg_mask = self.create_trg_mask(trg)\n",
    "        \n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        \n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "                \n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        #output = [batch size, trg len, output dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b26e1a99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T11:08:43.951809Z",
     "start_time": "2021-06-15T11:08:43.629789Z"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_SIZE = len(SRC.vocab)\n",
    "OUTPUT_SIZE = len(TRG.vocab)\n",
    "EMBEDDING_SIZE = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "N_HEADS = 8\n",
    "PF_SIZE = 512\n",
    "MAX_LENGTH = 100\n",
    "ENC_DROPOUT = 0.3\n",
    "DEC_DROPOUT = 0.3\n",
    "# input_size, embedding_size, pf_dim, n_layers, n_heads, max_length = 100, p_drop=0.5\n",
    "encoder = Encoder(INPUT_SIZE, EMBEDDING_SIZE, PF_SIZE, ENC_LAYERS, N_HEADS, MAX_LENGTH, ENC_DROPOUT).to(device)\n",
    "decoder = Decoder(OUTPUT_SIZE, EMBEDDING_SIZE, PF_SIZE, DEC_LAYERS, N_HEADS, MAX_LENGTH, DEC_DROPOUT).to(device)\n",
    "transformer = Transformer(encoder, decoder).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ecb743a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T11:08:44.139840Z",
     "start_time": "2021-06-15T11:08:43.953807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 9,542,061 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(transformer):,} trainable parameters')\n",
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "transformer.apply(initialize_weights);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb11f017",
   "metadata": {},
   "source": [
    "## i have chosen learning rate with 0.001 and 0.0005, but the valid loss not decreasing while the train loss is still going down, which means my model is overfitting, so i choose 0.0001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6f3b17e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T11:08:44.155842Z",
     "start_time": "2021-06-15T11:08:44.145845Z"
    }
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr = LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "best_valid_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "201649d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T11:08:44.203812Z",
     "start_time": "2021-06-15T11:08:44.158853Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, _ = transformer(src, trg[:,:-1])\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:,1:].contiguous().view(-1)\n",
    "    \n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "            #output = [batch size, trg len - 1, output dim]\n",
    "            #trg = [batch size, trg len]\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            #output = [batch size * trg len - 1, output dim]\n",
    "            #trg = [batch size * trg len - 1]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76b8e705",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T11:15:46.132931Z",
     "start_time": "2021-06-15T11:08:44.205834Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch: 01 | Time: 1m 42s\n",
      "\tTrain Loss: 5.486 | Train PPL: 241.313\n",
      "\t Val. Loss: 4.424 |  Val. PPL:  83.442\n",
      "1\n",
      "Epoch: 02 | Time: 1m 45s\n",
      "\tTrain Loss: 4.256 | Train PPL:  70.494\n",
      "\t Val. Loss: 4.069 |  Val. PPL:  58.519\n",
      "2\n",
      "Epoch: 03 | Time: 1m 45s\n",
      "\tTrain Loss: 3.925 | Train PPL:  50.649\n",
      "\t Val. Loss: 3.811 |  Val. PPL:  45.194\n",
      "3\n",
      "Epoch: 04 | Time: 1m 41s\n",
      "\tTrain Loss: 3.708 | Train PPL:  40.753\n",
      "\t Val. Loss: 3.657 |  Val. PPL:  38.753\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-03ca1583c05d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mtrain_loss_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-76be9e111b2c>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0msrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mtrg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchtext\\legacy\\data\\iterator.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    158\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m                         \u001b[0mminibatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m                 \u001b[1;32myield\u001b[0m \u001b[0mBatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchtext\\legacy\\data\\batch.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, dataset, device)\u001b[0m\n\u001b[0;32m     32\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mfield\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m                     \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m                     \u001b[0msetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchtext\\legacy\\data\\field.py\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(self, batch, device)\u001b[0m\n\u001b[0;32m    229\u001b[0m         \"\"\"\n\u001b[0;32m    230\u001b[0m         \u001b[0mpadded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m         \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumericalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchtext\\legacy\\data\\field.py\u001b[0m in \u001b[0;36mnumericalize\u001b[1;34m(self, arr, device)\u001b[0m\n\u001b[0;32m    351\u001b[0m                 \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpostprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m         \u001b[0mvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "valid_loss_arr = []\n",
    "train_loss_arr =[]\n",
    "\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(transformer, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(transformer, valid_iterator, criterion)\n",
    "    train_loss_arr.append(train_loss)\n",
    "    valid_loss_arr.append(valid_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        print(epoch)\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(transformer.state_dict(), 'transformer_traslation.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf403aec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T11:15:46.137910Z",
     "start_time": "2021-06-15T11:08:21.894Z"
    }
   },
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\n",
    "    \n",
    "    model.eval()\n",
    "        \n",
    "    if isinstance(sentence, str):\n",
    "        nlp = spacy.load('de_core_news_sm')\n",
    "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "\n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "        \n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "    \n",
    "    src_mask = model.create_src_mask(src_tensor)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        enc_src = model.encoder(src_tensor, src_mask)\n",
    "\n",
    "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
    "\n",
    "    for i in range(max_len):\n",
    "\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "        trg_mask = model.create_trg_mask(trg_tensor)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        pred_token = output.argmax(2)[:,-1].item()\n",
    "        \n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "            break\n",
    "    \n",
    "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
    "    \n",
    "    return trg_tokens[1:], attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2d6bd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T11:15:46.139915Z",
     "start_time": "2021-06-15T11:08:21.896Z"
    }
   },
   "outputs": [],
   "source": [
    "example_idx = 8\n",
    "\n",
    "src = vars(train_data.examples[example_idx])['src']\n",
    "trg = vars(train_data.examples[example_idx])['trg']\n",
    "\n",
    "print(f'src = {src}')\n",
    "print(f'trg = {trg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bf352a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T11:15:46.141930Z",
     "start_time": "2021-06-15T11:08:21.898Z"
    }
   },
   "outputs": [],
   "source": [
    "translation, attention = translate_sentence(src, SRC, TRG, transformer, device)\n",
    "\n",
    "print(f'predicted trg = {translation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f276f09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch (conda)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
