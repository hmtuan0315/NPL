{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This project is about translate english to vietnamese using seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T17:35:55.866369Z",
     "start_time": "2021-06-10T17:35:55.846416Z"
    }
   },
   "outputs": [],
   "source": [
    "%config IPCompleter.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T17:36:01.142344Z",
     "start_time": "2021-06-10T17:35:56.334412Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyvi import ViTokenizer, ViPosTagger, ViUtils\n",
    "import pyvi\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "%run contractions.ipynb\n",
    "%run ../Utils.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After EDA, i see the english data dont match with the vietnamese data, so i have modified the data then you can download 2 file \"modified_vi.txt\" and \"modified_en.txt\" and use it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:28:39.253736Z",
     "start_time": "2021-06-09T17:28:38.766680Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vi_df = pd.read_csv('modified_vi.txt', delimiter = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:28:39.551685Z",
     "start_time": "2021-06-09T17:28:39.256682Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eng_df = pd.read_csv('modified_en.txt', delimiter = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can use origin data from this [github link](https://github.com/stefan-it/nmt-en-vi)\n",
    "## Read file, uncomment the code below and run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:28:39.567688Z",
     "start_time": "2021-06-09T17:28:39.553681Z"
    }
   },
   "outputs": [],
   "source": [
    "# df.Col[469:-1] = df.Col[470:] \n",
    "# df.Col[8694:-1] = df.Col[8695:]\n",
    "# df.Col[9760:-1] = df.Col[9761:] \n",
    "# df.Col[10704:-1] = df.Col[10705:] \n",
    "# df.Col[21734:-1] = df.Col[21735:] \n",
    "# df.Col[26403:-1] = df.Col[26404:] \n",
    "# df.Col[29488:-1] = df.Col[29489:] \n",
    "# df.Col[38593:-1] = df.Col[38594:] \n",
    "# df.Col[41009:-1] = df.Col[41010:]\n",
    "# df.Col[48816:-1] = df.Col[48817:]\n",
    "# df.Col[50884:-1] = df.Col[50885:]\n",
    "# df.Col[51575:-1] = df.Col[51576:]\n",
    "# df.Col[54146:-1] = df.Col[54147:]\n",
    "# df.Col[56284:-1] = df.Col[56285:]\n",
    "# df.Col[57126:-1] = df.Col[57127:]\n",
    "# df.Col[57731:-1] = df.Col[57732:]\n",
    "# df.Col[58314:-1] = df.Col[58315:]\n",
    "# df.Col[66243:-1] = df.Col[66244:]\n",
    "# df.Col[68737:-1] = df.Col[68738:]\n",
    "# df.Col[73559:-1] = df.Col[73560:]\n",
    "# df.Col[76876:-1] = df.Col[76877:]\n",
    "# df.Col[77167:-1] = df.Col[77168:]\n",
    "# df.Col[79647:-1] = df.Col[79648:]\n",
    "# df.Col[87589:-1] = df.Col[87590:]\n",
    "# df.Col[88613:-1] = df.Col[88614:]\n",
    "# df.Col[89906:-1] = df.Col[89907:]\n",
    "# df.Col[94034:-1] = df.Col[94035:]\n",
    "# df.Col[104098:-1] = df.Col[104099:]\n",
    "# df.Col[104649:-1] = df.Col[104650:]\n",
    "# df.Col[106977:-1] = df.Col[106978:]\n",
    "# df.Col[112793:-1] = df.Col[112794:]\n",
    "# df.Col[115151:-1] = df.Col[115152:]\n",
    "# eng_df.Col[121582:-1] = eng_df.Col[121583:]\n",
    "# df.Col[123042:-1] = df.Col[123043:]\n",
    "# df.Col[124883:-1] = df.Col[124884:]\n",
    "# df.Col[125175:-1] = df.Col[125176:]\n",
    "# df.Col[127990:-1] = df.Col[127991:]\n",
    "# df.Col[130085:-1] = df.Col[130086:]\n",
    "# df.Col[130733:-1] = df.Col[130734:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:28:39.583679Z",
     "start_time": "2021-06-09T17:28:39.572686Z"
    }
   },
   "outputs": [],
   "source": [
    "# lines = new_en_df.values\n",
    "# with open('modified_en.txt', 'w', encoding='utf-8') as f:\n",
    "#     f.write('\\n'.join(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset is about 130k rows, but i only use 50k rows for time saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:28:39.599682Z",
     "start_time": "2021-06-09T17:28:39.586682Z"
    }
   },
   "outputs": [],
   "source": [
    "eng_sents = eng_df.Col.values[0:50000]\n",
    "vi_sents = vi_df.Col.values[0:50000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:28:42.969683Z",
     "start_time": "2021-06-09T17:28:39.604680Z"
    }
   },
   "outputs": [],
   "source": [
    "def expand_contractions(s, contractions_dict=CONTRACTION_MAP):\n",
    "    c_re = re.compile('(%s)' % '|'.join(CONTRACTION_MAP.keys()))\n",
    "    def replace(match):\n",
    "        return contractions_dict[match.group(0)]\n",
    "    return c_re.sub(replace, s)\n",
    "\n",
    "def eng_preprocessing(text):\n",
    "    text = text.lower()\n",
    "    #remove html tag\n",
    "    text = re.sub('\\s\\B&apos;', \"'\", text)\n",
    "    text = re.sub('\\s\\B&quot;', '', text)\n",
    "    #contractions\n",
    "    text = expand_contractions(text)\n",
    "    #remove special character\n",
    "    text = re.sub(\"[^a-zA-z0-9\\s]\", ' ',text)\n",
    "    #add sos and eos token\n",
    "    text = \" \".join(['SOS', text, 'EOS'])\n",
    "    return text\n",
    "eng_sents = list(map(eng_preprocessing, eng_sents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:28:43.819703Z",
     "start_time": "2021-06-09T17:28:42.974682Z"
    }
   },
   "outputs": [],
   "source": [
    "def vi_preprocessing(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\s\\B&apos;', \"'\", text)\n",
    "    text = re.sub('\\s\\B&quot;', '', text)\n",
    "    text = re.sub(r\"[!@#$%~&-/?><.,;:{}+_`|^=]\", ' ', text)\n",
    "    text = \" \".join(['SOS', text, 'EOS'])\n",
    "    return text\n",
    "vi_sents = list(map(vi_preprocessing, vi_sents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:28:43.835714Z",
     "start_time": "2021-06-09T17:28:43.822683Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SOS rachel pike   the science behind a climate headline EOS',\n",
       " 'SOS in 4 minutes   atmospheric chemist rachel pike provides a glimpse of the massive scientific effort behind the bold headlines on climate change   with her team    one of thousands who contributed    taking a risky flight over the rainforest in pursuit of data on a key molecule   EOS',\n",
       " 'SOS i would like to talk to you today about the scale of the scientific effort that goes into making the headlines you see in the paper   EOS',\n",
       " 'SOS headlines that look like this when they have to do with climate change   and headlines that look like this when they have to do with air quality or smog   EOS',\n",
       " 'SOS they are both two branches of the same field of atmospheric science   EOS']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_sents[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:28:43.867706Z",
     "start_time": "2021-06-09T17:28:43.838682Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SOS các tiêu đề gần đây trông như thế này khi ban điều hành biến đổi khí hậu liên chính phủ   gọi tắt là ipcc đưa ra bài nghiên cứu của họ về hệ thống khí quyển EOS',\n",
       " 'SOS nghiên cứu được viết bởi 620 nhà khoa học từ 40 quốc gia khác nhau EOS',\n",
       " 'SOS họ viết gần 1000 trang về chủ đề này EOS',\n",
       " 'SOS và tất cả các trang đều được xem xét bởi 400 khoa học gia và nhà phê bình khác từ 113 quốc gia EOS',\n",
       " 'SOS đó là cả một cộng đồng lớn   lớn đến nỗi trên thực tế cuộc tụ hội hằng năm của chúng tôi là hội nghị khoa học   91  tự nhiên   93  lớn nhất thế giới EOS']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vi_sents[5:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I use package pyvi for tokenize vietnamese text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:28:43.946712Z",
     "start_time": "2021-06-09T17:28:43.870704Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOS Ny\n",
      "nghiên_cứu V\n",
      "được V\n",
      "viết V\n",
      "bởi E\n",
      "620 M\n",
      "nhà N\n",
      "khoa_học N\n",
      "từ E\n",
      "40 M\n",
      "quốc_gia N\n",
      "khác A\n",
      "nhau N\n",
      "EOS Np\n"
     ]
    }
   ],
   "source": [
    "text = vi_sents[6]\n",
    "for ind in range(len(ViTokenizer.tokenize(text).split())):\n",
    "    print(ViPosTagger.postagging(ViTokenizer.tokenize(text))[0][ind], ViPosTagger.postagging(ViTokenizer.tokenize(text))[1][ind])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:29:19.428227Z",
     "start_time": "2021-06-09T17:28:43.949683Z"
    }
   },
   "outputs": [],
   "source": [
    "vi_tok = [ViTokenizer.tokenize(text).split() for text in vi_sents]\n",
    "eng_tok = [text.split() for text in eng_sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create word2index Dictionary for encoding sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:29:27.424259Z",
     "start_time": "2021-06-09T17:29:19.431226Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "eng_vocal = corpora.Dictionary(eng_tok)\n",
    "vi_vocal = corpora.Dictionary(vi_tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The pad token has the value 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:29:27.456207Z",
     "start_time": "2021-06-09T17:29:27.426265Z"
    }
   },
   "outputs": [],
   "source": [
    "special_tokens = {'PAD': 0}\n",
    "eng_vocal.patch_with_special_tokens(special_tokens)\n",
    "vi_vocal.patch_with_special_tokens(special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:29:27.487236Z",
     "start_time": "2021-06-09T17:29:27.459206Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26990"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eng_vocal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:29:27.502205Z",
     "start_time": "2021-06-09T17:29:27.490207Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20653"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vi_vocal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:29:27.533207Z",
     "start_time": "2021-06-09T17:29:27.505205Z"
    }
   },
   "outputs": [],
   "source": [
    "len_vi_sent = [len(text) for text in vi_tok]\n",
    "len_eng_sent = [len(text) for text in eng_tok]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:29:27.548205Z",
     "start_time": "2021-06-09T17:29:27.536206Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3989\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for sent in len_vi_sent:\n",
    "    if sent >= 40:\n",
    "        count = count+1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:29:28.180205Z",
     "start_time": "2021-06-09T17:29:27.550207Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x1e5398ba648>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(x=len_vi_sent, binwidth=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:29:29.989792Z",
     "start_time": "2021-06-09T17:29:28.183211Z"
    }
   },
   "outputs": [],
   "source": [
    "def eng_encode_sent(sent):\n",
    "    sent = [eng_vocal.token2id[word] for word in sent if word in eng_vocal.token2id.keys()]\n",
    "    return sent\n",
    "def vi_encode_sent(sent):\n",
    "    sent = [vi_vocal.token2id[word] for word in sent if word in vi_vocal.token2id.keys()]\n",
    "    return sent\n",
    "eng_encoded = list(map(eng_encode_sent, eng_tok))\n",
    "vi_encoded = list(map(vi_encode_sent, vi_tok))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding encoded sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:29:31.457691Z",
     "start_time": "2021-06-09T17:29:29.992751Z"
    }
   },
   "outputs": [],
   "source": [
    "def pad_features(encoded_sent, seq_length):\n",
    "    features = np.zeros((len(encoded_sent), seq_length), dtype = int)\n",
    "    \n",
    "    for i, review in enumerate(encoded_sent):\n",
    "        review_len = len(review)\n",
    "        if review_len <= seq_length:\n",
    "            zeroes = list(np.zeros(seq_length-review_len))\n",
    "            new = review+zeroes\n",
    "        elif review_len > seq_length:\n",
    "            new = review[0:seq_length]\n",
    "        features[i,:] = np.array(new)  \n",
    "    return features\n",
    "\n",
    "eng_encoded = pad_features(eng_encoded, 40)\n",
    "vi_encoded = pad_features(vi_encoded, 40)\n",
    "\n",
    "X = eng_encoded\n",
    "y = vi_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:29:31.473695Z",
     "start_time": "2021-06-09T17:29:31.465693Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    1,     7,     6,     9,     8,     3,     2,     4,     5,\n",
       "       26989,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_encoded[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test split and batch loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:29:31.536722Z",
     "start_time": "2021-06-09T17:29:31.478699Z"
    }
   },
   "outputs": [],
   "source": [
    "rate = 0.1/0.9\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=rate, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:29:31.551692Z",
     "start_time": "2021-06-09T17:29:31.539692Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "train_data = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "valid_data = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
    "test_data = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:29:31.598727Z",
     "start_time": "2021-06-09T17:29:31.553693Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:29:31.614690Z",
     "start_time": "2021-06-09T17:29:31.600751Z"
    }
   },
   "outputs": [],
   "source": [
    "# for x, y in train_loader:\n",
    "#     print(x.size())\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create encoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### each word of source sentence will be the input of lstm cell, then the lstm cell outputs hidden cell and its output. \n",
    "### The last hidden cell is the input of encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:29:31.630693Z",
     "start_time": "2021-06-09T17:29:31.617691Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_dim, hidden_dim, p_drop, n_layers):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.p_drop = p_drop\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, batch_first=True, dropout=p_drop)\n",
    "        self.dropout = nn.Dropout(p_drop)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "\n",
    "        \n",
    "        return hidden\n",
    "    \n",
    "    def forward(self, x, h):\n",
    "        #x: [batch_size, seq_len]\n",
    "        out = self.embedding(x)\n",
    "        #out: [batch_size, seq_len, embedding_dim]\n",
    "        out = self.dropout(out)\n",
    "        out, h = self.lstm(out, h)\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:29:34.793425Z",
     "start_time": "2021-06-09T17:29:31.633696Z"
    }
   },
   "outputs": [],
   "source": [
    "ENCODER_INPUT_SIZE = len(eng_vocal)\n",
    "ENCODER_EMBEDDING_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "N_LAYERS = 2\n",
    "P_DROP = 0.5\n",
    "\n",
    "encoder = Encoder(ENCODER_INPUT_SIZE, ENCODER_EMBEDDING_DIM, HIDDEN_DIM,P_DROP, N_LAYERS)\n",
    "encoder = encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:29:34.809420Z",
     "start_time": "2021-06-09T17:29:34.797423Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# for x, y in train_loader:\n",
    "#     if torch.cuda.is_available():\n",
    "#         x = x.cuda()\n",
    "#         y = y.cuda()\n",
    "#     h = encoder.init_hidden(batch_size)\n",
    "#     out, h = encoder(x, h)\n",
    "#     print(h[0].size())\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Decoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the len of input in each lstm cell of decoder is 1, because we fetch only 1 word to predict the next word\n",
    "### i use teacher forcing to make convergence faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:29:34.825455Z",
     "start_time": "2021-06-09T17:29:34.812418Z"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, embedding_dim, hidden_dim, p_drop, n_layers):\n",
    "        super().__init__()\n",
    "        self.output_size = output_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.p_drop = p_drop\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, embedding_dim)\n",
    "        self.dropout = nn.Dropout(p=p_drop)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, batch_first=True, dropout = p_drop)\n",
    "        self.fc1 = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    def forward(self, x, h):\n",
    "        out = x.unsqueeze(1)\n",
    "        #out: [batch_size = 1, seq_len]\n",
    "        out = self.embedding(out)\n",
    "        #out: [1, seq_len, embedding_dim]\n",
    "        out = self.dropout(out)\n",
    "        out, h = self.lstm(out, h)\n",
    "        #out: [1, seq_len, hidden_dim]\n",
    "        out = out.reshape(-1, self.hidden_dim)\n",
    "        #out: [seq_len, hidden_dim]\n",
    "        out = self.fc1(out)\n",
    "        #out: [seq_len, output_size]\n",
    "        return out, h\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:29:35.237475Z",
     "start_time": "2021-06-09T17:29:34.827461Z"
    }
   },
   "outputs": [],
   "source": [
    "DECODER_OUTPUT_SIZE = len(vi_vocal)\n",
    "DECODER_EMBEDDING_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "N_LAYERS = 2\n",
    "P_DROP = 0.5\n",
    "\n",
    "decoder = Decoder(DECODER_OUTPUT_SIZE, DECODER_EMBEDDING_DIM, HIDDEN_DIM,P_DROP, N_LAYERS)\n",
    "decoder = decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:29:35.253455Z",
     "start_time": "2021-06-09T17:29:35.240422Z"
    }
   },
   "outputs": [],
   "source": [
    "# for x, y in train_loader:\n",
    "#     if torch.cuda.is_available():\n",
    "#         x = x.cuda()\n",
    "#         y = y.cuda()\n",
    "#     input =y[:,1]\n",
    "#     out1 = decoder(input, h)\n",
    "#     print(out1.size())\n",
    "# #     print(input[:,0].size())\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:29:35.269464Z",
     "start_time": "2021-06-09T17:29:35.255452Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder= encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        trg_len = trg.size(1)\n",
    "        trg_batch = trg.size(0)\n",
    "        #tensor to store outputs\n",
    "        outputs = torch.zeros(trg_len, trg_batch, DECODER_OUTPUT_SIZE)\n",
    "        outputs = outputs.to(device)\n",
    "        hidden = self.encoder.init_hidden(trg_batch)\n",
    "        h = self.encoder(src, hidden)\n",
    "        #We use the first word of target data SOS to fetch into decoder\n",
    "        input = trg[:, 0]\n",
    "        \n",
    "        for i in range(1, trg_len):\n",
    "            output, h = self.decoder(input, h)\n",
    "            outputs[i] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1) \n",
    "            input = trg[:,i] if teacher_force else top1\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:29:35.285419Z",
     "start_time": "2021-06-09T17:29:35.272426Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(26990, 256)\n",
       "    (lstm): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(20653, 256)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (lstm): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
       "    (fc1): Linear(in_features=512, out_features=20653, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_model = Seq2Seq(encoder, decoder)\n",
    "seq_model = seq_model.to(device)\n",
    "seq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:29:35.301423Z",
     "start_time": "2021-06-09T17:29:35.287420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 30,148,013 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(seq_model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:29:35.317422Z",
     "start_time": "2021-06-09T17:29:35.304419Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T17:34:24.654268Z",
     "start_time": "2021-06-10T17:34:24.642285Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "optimizer = torch.optim.Adam(seq_model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "clip = 5\n",
    "epouchs = 3\n",
    "\n",
    "train_loss_arr = []\n",
    "\n",
    "seq_model.train()\n",
    "#     model.eval()\n",
    "for epouch in range(epouchs):\n",
    "    epoch_loss = 0\n",
    "    count = 0\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        batch = y.size(0)\n",
    "        output = seq_model(x, y)\n",
    "\n",
    "        output_dim = output.size(2)\n",
    "        output = output[1:].reshape(-1, output_dim)\n",
    "\n",
    "        y = y.reshape(-1, batch)\n",
    "        trg = y[1:].reshape(-1)\n",
    "\n",
    "        loss = criterion(output, trg.long().to(device))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(seq_model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "#             if count % 100 ==0:\n",
    "        print(count)\n",
    "        count = count + 1\n",
    "    print(f'Epouch: {epouch + 1}, Train Loss: {epoch_loss / len(train_loader)}')\n",
    "    train_loss_arr.append(epoch_loss / len(train_loader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict unseen sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T17:36:27.935900Z",
     "start_time": "2021-06-10T17:36:27.926898Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1571, 2730]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"beautiful girl\"\n",
    "def predict(text, max_length):\n",
    "    text = \" \".join(['SOS', text, 'EOS'])\n",
    "    text = [eng1_encode_sent(text)]\n",
    "    text = pad_features(text, 40)\n",
    "    text = torch.from_numpy(np.array(text))\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        text = text.cuda()\n",
    "    outputs = [vi_vocal.token2id['SOS']]\n",
    "    h = seq_model.encoder.init_hidden(1)\n",
    "    seq_model.eval()\n",
    "    with torch.no_grad():\n",
    "        h = seq_model.encoder(text, h)\n",
    "     \n",
    "    for i in range(max_length):\n",
    "        previous_word = torch.LongTensor([outputs[-1]]).cuda()\n",
    "        with torch.no_grad():\n",
    "            output, h = seq_model.decoder(previous_word, h)\n",
    "#             return output\n",
    "            best_guess = output.argmax(1).item()\n",
    "        outputs.append(best_guess)\n",
    "\n",
    "        Model predicts it's the end of the sentence\n",
    "        if output.argmax(1).item() == vi_vocal.token2id['EOS']:\n",
    "            break\n",
    "    \n",
    "    return outputs\n",
    "predict(text,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T18:37:31.108052Z",
     "start_time": "2021-06-09T18:37:31.091051Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1571, 2730]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def eng1_encode_sent(sent):\n",
    "    new = [eng_vocal.token2id[word] for word in sent.split() if word in eng_vocal.token2id.keys()]\n",
    "    return new\n",
    "\n",
    "eng1_encode_sent(\"beautiful girl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch (conda)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
