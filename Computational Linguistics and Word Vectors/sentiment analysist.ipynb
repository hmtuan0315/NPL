{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3e90e49",
   "metadata": {},
   "source": [
    "## If you want to download datasets and trained models, go to this [link](https://drive.google.com/drive/folders/1Ltqj8wiuXxrl1p_KzrzfaJkn2EDfI-M4?usp=sharing) for convenience. Download file named word2vec sentiment analysys.zip and unzip it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea3921e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T14:48:53.771703Z",
     "start_time": "2021-06-10T14:48:53.745753Z"
    }
   },
   "outputs": [],
   "source": [
    "%config IPCompleter.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d876c0a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T13:25:21.841405Z",
     "start_time": "2021-06-10T13:25:05.755272Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "import spacy\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import unidecode\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import word2vec\n",
    "import numpy as np\n",
    "\n",
    "%run contractions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85050d0c",
   "metadata": {},
   "source": [
    "**Import origin tweet dataset without preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a624b85d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T13:30:15.199233Z",
     "start_time": "2021-06-10T13:30:15.188270Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./pytorch dataset and model/tweet data.csv\", encoding = \"ISO-8859-1\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa462ed0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T13:18:08.315838Z",
     "start_time": "2021-06-10T13:18:08.309860Z"
    }
   },
   "source": [
    "**Import saved dataset after preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fda24f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T13:30:11.713258Z",
     "start_time": "2021-06-10T13:30:06.341242Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can not update his facebook b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>i dived many time for the ball managed to save...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feel itchy and like it on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>no it is not behaving at all i am mad why am i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596816</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>just woke up having no school is the best feel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596817</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>thewdbcom very cool to hear old walt interview a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596818</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>are you ready for your mojo makeover ask me fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596819</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>happy th birthday to my boo of alll time tupac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596820</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy charitytuesday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1596821 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         target          id                          date      flag  \\\n",
       "0             0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "1             0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "2             0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "3             0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4             0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY   \n",
       "...         ...         ...                           ...       ...   \n",
       "1596816       4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1596817       4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1596818       4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1596819       4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1596820       4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    user                                               text  \n",
       "0          scotthamilton  is upset that he can not update his facebook b...  \n",
       "1               mattycus  i dived many time for the ball managed to save...  \n",
       "2                ElleCTF       my whole body feel itchy and like it on fire  \n",
       "3                 Karoli  no it is not behaving at all i am mad why am i...  \n",
       "4               joy_wolf                                 not the whole crew  \n",
       "...                  ...                                                ...  \n",
       "1596816  AmandaMarie1028  just woke up having no school is the best feel...  \n",
       "1596817      TheWDBoards   thewdbcom very cool to hear old walt interview a  \n",
       "1596818           bpbabe  are you ready for your mojo makeover ask me fo...  \n",
       "1596819     tinydiamondz  happy th birthday to my boo of alll time tupac...  \n",
       "1596820   RyanTrevMorris                               happy charitytuesday  \n",
       "\n",
       "[1596821 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./pytorch dataset and model/lastFinalTweet.csv\", encoding = \"ISO-8859-1\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0dacd11e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T13:30:32.547458Z",
     "start_time": "2021-06-10T13:30:32.523479Z"
    }
   },
   "outputs": [],
   "source": [
    "review = df.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e6abc8",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5f5b25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T03:28:21.628642Z",
     "start_time": "2021-05-18T03:27:46.981294Z"
    }
   },
   "source": [
    "## 1) remove spacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a05f937",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T12:37:37.048205Z",
     "start_time": "2021-05-18T12:37:33.703225Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_spacing(text):\n",
    "    res = \" \".join(text.split())\n",
    "    return res\n",
    "review = list(map(remove_spacing, review))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882fdf5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T03:33:34.195531Z",
     "start_time": "2021-05-18T03:33:34.189559Z"
    }
   },
   "source": [
    "## 2) remove accent characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca25af7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T07:45:59.918779Z",
     "start_time": "2021-05-18T07:45:56.871508Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_accent_char(text):\n",
    "    res = unidecode.unidecode(text)\n",
    "    return res\n",
    "review = list(map(remove_accent_char, review))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d41065",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T03:36:51.894929Z",
     "start_time": "2021-05-18T03:36:51.885962Z"
    }
   },
   "source": [
    "## 3) Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31200c6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T07:46:44.383766Z",
     "start_time": "2021-05-18T07:45:59.923760Z"
    }
   },
   "outputs": [],
   "source": [
    "c_re = re.compile('(%s)' % '|'.join(CONTRACTION_MAP.keys()))\n",
    "def expand_contractions(s, contractions_dict=CONTRACTION_MAP):\n",
    "    def replace(match):\n",
    "        return contractions_dict[match.group(0)]\n",
    "    return c_re.sub(replace, s)\n",
    "review = list(map(expand_contractions, review))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba78d2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T04:23:19.990570Z",
     "start_time": "2021-05-18T04:23:19.982621Z"
    }
   },
   "source": [
    "## 4) Remove special character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e76c96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T12:51:13.209389Z",
     "start_time": "2021-05-18T12:50:30.432421Z"
    }
   },
   "outputs": [],
   "source": [
    "# def remove_special_characters(text, remove_digits=True):\n",
    "#     pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "#     text = re.sub(pattern, '', text)\n",
    "#     return text\n",
    "# review = list(map(expand_contractions, review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd242edf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T07:47:29.816470Z",
     "start_time": "2021-05-18T07:47:29.802504Z"
    }
   },
   "outputs": [],
   "source": [
    "text =\"sorry! bed time came here (GMT+1) http://is.gd/fNge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733153c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T07:47:34.782500Z",
     "start_time": "2021-05-18T07:47:29.819470Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_tweetName(text):\n",
    "    res = re.sub(\"@\\w+\",\"\", text)\n",
    "    return res\n",
    "review = list(map(remove_tweetName, review))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a68d7c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T07:47:39.098842Z",
     "start_time": "2021-05-18T07:47:34.785498Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_httpLink(text):\n",
    "    res = re.sub(\"http\\S+\", \"\", text)\n",
    "    return res\n",
    "review = list(map(remove_httpLink, review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110e8168",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T12:54:10.572318Z",
     "start_time": "2021-05-18T12:54:05.136279Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_special_characters(text, remove_digits=True):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "review = list(map(remove_special_characters, review))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80970cc7",
   "metadata": {},
   "source": [
    "## 5) remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d40ca2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T07:47:46.859793Z",
     "start_time": "2021-05-18T07:47:46.846814Z"
    }
   },
   "outputs": [],
   "source": [
    "stopwordList = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    filted_sentence = \" \".join([word for word in text.split() if word not in stopwordList])\n",
    "    return filted_sentence\n",
    "review = list(map(remove_stopwords, review))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d758eb3",
   "metadata": {},
   "source": [
    "## 6) remove nan value after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f83883d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T16:58:46.151838Z",
     "start_time": "2021-05-18T16:58:44.847212Z"
    }
   },
   "outputs": [],
   "source": [
    "def isNaN(string):\n",
    "    return string != string\n",
    "\n",
    "nan_index =[]\n",
    "for ind, sent in enumerate(review):\n",
    "    if isNaN(sent):\n",
    "        nan_index.append(ind)\n",
    "df = df.drop(nan_index)\n",
    "df = df.reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189ac12b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T16:44:40.961427Z",
     "start_time": "2021-05-18T16:44:40.943457Z"
    }
   },
   "outputs": [],
   "source": [
    "test_review = review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca1d786",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T07:51:54.396875Z",
     "start_time": "2021-05-18T07:51:53.613548Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7b37ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T07:47:48.046792Z",
     "start_time": "2021-05-18T07:47:48.033844Z"
    }
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3202cb67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T07:50:08.184831Z",
     "start_time": "2021-05-18T07:50:08.172850Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = [\"tok2vec\", \"tagger\", \"parser\", \"ner\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a871a857",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T07:56:38.241138Z",
     "start_time": "2021-05-18T07:56:38.221145Z"
    }
   },
   "outputs": [],
   "source": [
    "for doc in nlp.pipe(texts, batch_size=2000, disable=[\"transformer\",\"tok2vec\" ]):\n",
    "    # Do something with the doc here\n",
    "    print([(ent.lemma_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddfd90f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T12:52:13.145391Z",
     "start_time": "2021-05-18T12:52:12.433390Z"
    }
   },
   "outputs": [],
   "source": [
    "review = list(map(lambda text: text.lower(), review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833a16ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T13:29:12.210739Z",
     "start_time": "2021-05-18T13:29:11.952714Z"
    }
   },
   "outputs": [],
   "source": [
    "df.text = review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02f50b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:12:24.819244Z",
     "start_time": "2021-05-18T15:05:33.442277Z"
    }
   },
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    text = nltk.word_tokenize(text)\n",
    "    return text\n",
    "\n",
    "iter = round(len(review) / 10000)\n",
    "batch_size = 10000\n",
    "res_arr = []\n",
    "for i in range(1, iter + 1, 1):\n",
    "    print(i)\n",
    "    res = list(map(lemmatize_text, review[((i-1)*batch_size) : (i)* batch_size]))\n",
    "    res_arr = res_arr + res\n",
    "\n",
    "res_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c745d1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:18:21.125525Z",
     "start_time": "2021-05-18T15:18:21.114541Z"
    }
   },
   "outputs": [],
   "source": [
    "fulltext = review\n",
    "tokentext = res_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7713212",
   "metadata": {},
   "source": [
    "# Tạo word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9997953",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:30:08.491740Z",
     "start_time": "2021-05-18T15:28:15.476135Z"
    }
   },
   "outputs": [],
   "source": [
    "word2vec_model = word2vec.Word2Vec(tokentext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd8206fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T13:31:56.095531Z",
     "start_time": "2021-06-10T13:31:56.083533Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x1fabd9f8048>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06981bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:32:10.293929Z",
     "start_time": "2021-05-18T15:32:10.252874Z"
    }
   },
   "outputs": [],
   "source": [
    "word2vec_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fed624b",
   "metadata": {},
   "source": [
    "**Save lại word2vec model sau khi train xong**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09afa416",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:34:01.790056Z",
     "start_time": "2021-05-18T15:34:01.077035Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"word2vec.model\"\n",
    "word2vec_model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cb9c7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T13:42:07.257378Z",
     "start_time": "2021-06-10T13:42:07.236406Z"
    }
   },
   "source": [
    "**Các từ gần nghĩa nhất với từ hero**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94338120",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T13:31:58.802040Z",
     "start_time": "2021-06-10T13:31:58.643111Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bass', 0.6834656000137329),\n",
       " ('teardrop', 0.6448805332183838),\n",
       " ('metallica', 0.5705269575119019),\n",
       " ('player', 0.5653506517410278),\n",
       " ('playing', 0.5631834864616394),\n",
       " ('guitar', 0.540151834487915),\n",
       " ('favourite', 0.5208714604377747),\n",
       " ('mario', 0.5193303227424622),\n",
       " ('teardrops', 0.5149649977684021),\n",
       " ('fave', 0.5140201449394226)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv.most_similar(\"hero\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "647ab81b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T13:32:03.501168Z",
     "start_time": "2021-06-10T13:32:03.459202Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'to',\n",
       " 'the',\n",
       " 'a',\n",
       " 'is',\n",
       " 'my',\n",
       " 'it',\n",
       " 'and',\n",
       " 'you',\n",
       " 'not',\n",
       " 'for',\n",
       " 'in',\n",
       " 'am',\n",
       " 'of',\n",
       " 'have',\n",
       " 'on',\n",
       " 'me',\n",
       " 'that',\n",
       " 'so',\n",
       " 'but',\n",
       " 'just',\n",
       " 'with',\n",
       " 'be',\n",
       " 'at',\n",
       " 'do',\n",
       " 'wa',\n",
       " 'are',\n",
       " 'day',\n",
       " 'will',\n",
       " 'this',\n",
       " 'now',\n",
       " 'good',\n",
       " 'up',\n",
       " 'can',\n",
       " 'get',\n",
       " 'all',\n",
       " 'out',\n",
       " 'like',\n",
       " 'go',\n",
       " 'no',\n",
       " 'got',\n",
       " 'u',\n",
       " 'love',\n",
       " 'work',\n",
       " 'today',\n",
       " 'your',\n",
       " 'too',\n",
       " 'going',\n",
       " 'time',\n",
       " 'we',\n",
       " 'back',\n",
       " 'from',\n",
       " 'one',\n",
       " 'what',\n",
       " 'lol',\n",
       " 'know',\n",
       " 'about',\n",
       " 'im',\n",
       " 'really',\n",
       " 'had',\n",
       " 'want',\n",
       " 'see',\n",
       " 'some',\n",
       " 'did',\n",
       " 'there',\n",
       " 'night',\n",
       " 'think',\n",
       " 'if',\n",
       " 'still',\n",
       " 'new',\n",
       " 'how',\n",
       " 'well',\n",
       " 'na',\n",
       " 'they',\n",
       " 'amp',\n",
       " 'would',\n",
       " 'need',\n",
       " 'thanks',\n",
       " 'home',\n",
       " 'when',\n",
       " 'ha',\n",
       " 'oh',\n",
       " 'more',\n",
       " 'miss',\n",
       " 'much',\n",
       " 'he',\n",
       " 'here',\n",
       " 'off',\n",
       " 'last',\n",
       " 'an',\n",
       " 'feel',\n",
       " 'hope',\n",
       " 'morning',\n",
       " 'then',\n",
       " 'make',\n",
       " 'been',\n",
       " 'tomorrow',\n",
       " 'great',\n",
       " 'twitter',\n",
       " 'or',\n",
       " 'her',\n",
       " 'haha',\n",
       " 'again',\n",
       " 'wish',\n",
       " 'its',\n",
       " 'she',\n",
       " 'sad',\n",
       " 'come',\n",
       " 'fun',\n",
       " 'only',\n",
       " 'why',\n",
       " 'right',\n",
       " 'week',\n",
       " 'sleep',\n",
       " 'bad',\n",
       " 'very',\n",
       " 'happy',\n",
       " 'could',\n",
       " 'sorry',\n",
       " 'cant',\n",
       " 'thing',\n",
       " 'by',\n",
       " 'tonight',\n",
       " 'way',\n",
       " 'friend',\n",
       " 'them',\n",
       " 'getting',\n",
       " 'gon',\n",
       " 'dont',\n",
       " 'should',\n",
       " 'though',\n",
       " 'over',\n",
       " 'nice',\n",
       " 'better',\n",
       " 'watching',\n",
       " 'wait',\n",
       " 'say',\n",
       " 'bed',\n",
       " 'yeah',\n",
       " 'hate',\n",
       " 'look',\n",
       " 'were',\n",
       " 'people',\n",
       " 'school',\n",
       " 'hour',\n",
       " 'him',\n",
       " 'weekend',\n",
       " 'even',\n",
       " 'yes',\n",
       " 'guy',\n",
       " 'after',\n",
       " 'hey',\n",
       " 'next',\n",
       " 'doe',\n",
       " 'take',\n",
       " 'show',\n",
       " 'down',\n",
       " 'who',\n",
       " 'awesome',\n",
       " 'never',\n",
       " 'thank',\n",
       " 'soon',\n",
       " 'tweet',\n",
       " 'little',\n",
       " 'long',\n",
       " 'wan',\n",
       " 'first',\n",
       " 'working',\n",
       " 'life',\n",
       " 'thats',\n",
       " 'best',\n",
       " 'please',\n",
       " 'year',\n",
       " 'doing',\n",
       " 'movie',\n",
       " 'having',\n",
       " 'being',\n",
       " 'tired',\n",
       " 'watch',\n",
       " 'sick',\n",
       " 'feeling',\n",
       " 'everyone',\n",
       " 'his',\n",
       " 'ok',\n",
       " 'any',\n",
       " 'our',\n",
       " 'done',\n",
       " 'let',\n",
       " 'always',\n",
       " 'sure',\n",
       " 'already',\n",
       " 'girl',\n",
       " 'than',\n",
       " 'find',\n",
       " 'x',\n",
       " 'another',\n",
       " 'where',\n",
       " 'cool',\n",
       " 'because',\n",
       " 'something',\n",
       " 'phone',\n",
       " 'ready',\n",
       " 'made',\n",
       " 'suck',\n",
       " 'looking',\n",
       " 'man',\n",
       " 'yay',\n",
       " 'lot',\n",
       " 'went',\n",
       " 'keep',\n",
       " 'ur',\n",
       " 'lt',\n",
       " 'song',\n",
       " 'yet',\n",
       " 'before',\n",
       " 'house',\n",
       " 'hurt',\n",
       " 'help',\n",
       " 'ever',\n",
       " 'start',\n",
       " 'pretty',\n",
       " 'thought',\n",
       " 'trying',\n",
       " 'away',\n",
       " 'summer',\n",
       " 'maybe',\n",
       " 'old',\n",
       " 'omg',\n",
       " 'finally',\n",
       " 'amazing',\n",
       " 'early',\n",
       " 'game',\n",
       " 'someone',\n",
       " 'into',\n",
       " 'bit',\n",
       " 'left',\n",
       " 'lost',\n",
       " 'guess',\n",
       " 'follow',\n",
       " 'damn',\n",
       " 'mean',\n",
       " 'rain',\n",
       " 'baby',\n",
       " 'big',\n",
       " 'missed',\n",
       " 'same',\n",
       " 'tell',\n",
       " 'hot',\n",
       " 'n',\n",
       " 'while',\n",
       " 'nothing',\n",
       " 'try',\n",
       " 'glad',\n",
       " 'wow',\n",
       " 'birthday',\n",
       " 'other',\n",
       " 'coming',\n",
       " 'also',\n",
       " 'party',\n",
       " 'bored',\n",
       " 'live',\n",
       " 'two',\n",
       " 'weather',\n",
       " 'pic',\n",
       " 'sound',\n",
       " 'sun',\n",
       " 'hear',\n",
       " 'later',\n",
       " 'play',\n",
       " 'stuff',\n",
       " 'actually',\n",
       " 'those',\n",
       " 'saw',\n",
       " 'ya',\n",
       " 'ta',\n",
       " 'might',\n",
       " 'excited',\n",
       " 'waiting',\n",
       " 'said',\n",
       " 'hard',\n",
       " 'call',\n",
       " 'mom',\n",
       " 'since',\n",
       " 'until',\n",
       " 'car',\n",
       " 'give',\n",
       " 'yesterday',\n",
       " 'th',\n",
       " 'd',\n",
       " 'exam',\n",
       " 'few',\n",
       " 'ugh',\n",
       " 'world',\n",
       " 'god',\n",
       " 'such',\n",
       " 'job',\n",
       " 'hi',\n",
       " 'head',\n",
       " 'late',\n",
       " 'around',\n",
       " 'myself',\n",
       " 'many',\n",
       " 'video',\n",
       " 'music',\n",
       " 'check',\n",
       " 'luck',\n",
       " 'found',\n",
       " 'friday',\n",
       " 'put',\n",
       " 'read',\n",
       " 'talk',\n",
       " 'sunday',\n",
       " 'their',\n",
       " 'cold',\n",
       " 'must',\n",
       " 'beautiful',\n",
       " 'didnt',\n",
       " 'making',\n",
       " 'stop',\n",
       " 'monday',\n",
       " 'gone',\n",
       " 'may',\n",
       " 'missing',\n",
       " 'aww',\n",
       " 'end',\n",
       " 'least',\n",
       " 'anything',\n",
       " 'woke',\n",
       " 'poor',\n",
       " 'follower',\n",
       " 'till',\n",
       " 'family',\n",
       " 'leave',\n",
       " 'use',\n",
       " 'most',\n",
       " 'almost',\n",
       " 'hair',\n",
       " 'food',\n",
       " 'okay',\n",
       " 'listening',\n",
       " 'tho',\n",
       " 'cute',\n",
       " 'boy',\n",
       " 'lunch',\n",
       " 'far',\n",
       " 'wanted',\n",
       " 'iphone',\n",
       " 'kid',\n",
       " 'eat',\n",
       " 'free',\n",
       " 'book',\n",
       " 'dinner',\n",
       " 'month',\n",
       " 'funny',\n",
       " 'enjoy',\n",
       " 'finished',\n",
       " 'picture',\n",
       " 'believe',\n",
       " 'sweet',\n",
       " 'class',\n",
       " 'playing',\n",
       " 'forward',\n",
       " 'place',\n",
       " 'shit',\n",
       " 'anyone',\n",
       " 'welcome',\n",
       " 's',\n",
       " 'without',\n",
       " 'thinking',\n",
       " 'everything',\n",
       " 'which',\n",
       " 'every',\n",
       " 'cry',\n",
       " 'mine',\n",
       " 'r',\n",
       " 'totally',\n",
       " 'win',\n",
       " 'buy',\n",
       " 'update',\n",
       " 'idea',\n",
       " 'these',\n",
       " 'outside',\n",
       " 'enough',\n",
       " 'w',\n",
       " 'stupid',\n",
       " 'real',\n",
       " 'ill',\n",
       " 'hahaha',\n",
       " 'b',\n",
       " 'through',\n",
       " 'coffee',\n",
       " 'stay',\n",
       " 'wrong',\n",
       " 'dad',\n",
       " 'anymore',\n",
       " 'room',\n",
       " 'probably',\n",
       " 'cause',\n",
       " 'dog',\n",
       " 'eating',\n",
       " 'once',\n",
       " 'money',\n",
       " 'name',\n",
       " 'xx',\n",
       " 'fan',\n",
       " 'tv',\n",
       " 'sooo',\n",
       " 'following',\n",
       " 'busy',\n",
       " 'minute',\n",
       " 'as',\n",
       " 'lovely',\n",
       " 'post',\n",
       " 'saturday',\n",
       " 'dream',\n",
       " 'whole',\n",
       " 'came',\n",
       " 'seen',\n",
       " 'headache',\n",
       " 'o',\n",
       " 'taking',\n",
       " 'kinda',\n",
       " 'beach',\n",
       " 'face',\n",
       " 'both',\n",
       " 'run',\n",
       " 'took',\n",
       " 'hopefully',\n",
       " 'crazy',\n",
       " 'super',\n",
       " 'hell',\n",
       " 'computer',\n",
       " 'word',\n",
       " 'hello',\n",
       " 'eye',\n",
       " 'able',\n",
       " 'news',\n",
       " 'half',\n",
       " 'meet',\n",
       " 'true',\n",
       " 'forgot',\n",
       " 'hit',\n",
       " 'blog',\n",
       " 'goodnight',\n",
       " 'pm',\n",
       " 'plan',\n",
       " 'awww',\n",
       " 'problem',\n",
       " 'leaving',\n",
       " 'part',\n",
       " 'sitting',\n",
       " 'reading',\n",
       " 'wont',\n",
       " 'rest',\n",
       " 'p',\n",
       " 'send',\n",
       " 'either',\n",
       " 'ago',\n",
       " 'used',\n",
       " 'final',\n",
       " 'else',\n",
       " 'trip',\n",
       " 'soo',\n",
       " 'full',\n",
       " 'shopping',\n",
       " 'youre',\n",
       " 'photo',\n",
       " 'kind',\n",
       " 'heart',\n",
       " 'brother',\n",
       " 'seems',\n",
       " 'ah',\n",
       " 'talking',\n",
       " 'cuz',\n",
       " 'watched',\n",
       " 'office',\n",
       " 'raining',\n",
       " 'mind',\n",
       " 'own',\n",
       " 'tried',\n",
       " 'remember',\n",
       " 'email',\n",
       " 'change',\n",
       " 'stuck',\n",
       " 'fuck',\n",
       " 'heard',\n",
       " 'alone',\n",
       " 'internet',\n",
       " 'link',\n",
       " 'break',\n",
       " 'sister',\n",
       " 'boo',\n",
       " 'course',\n",
       " 'started',\n",
       " 'rock',\n",
       " 'btw',\n",
       " 'reply',\n",
       " 'seeing',\n",
       " 'care',\n",
       " ']',\n",
       " 'hehe',\n",
       " 'site',\n",
       " 'pain',\n",
       " 'ticket',\n",
       " 'st',\n",
       " 'whats',\n",
       " 'using',\n",
       " 'la',\n",
       " 'quite',\n",
       " 'add',\n",
       " 'concert',\n",
       " 'drink',\n",
       " 'online',\n",
       " 'wake',\n",
       " 'told',\n",
       " 'season',\n",
       " 'awake',\n",
       " 'loved',\n",
       " 'person',\n",
       " 'fine',\n",
       " 'dude',\n",
       " 'text',\n",
       " 'til',\n",
       " 'breakfast',\n",
       " 'favorite',\n",
       " 'open',\n",
       " 'pay',\n",
       " 'bought',\n",
       " 'sunny',\n",
       " 'hug',\n",
       " 'boring',\n",
       " 'seriously',\n",
       " 'broke',\n",
       " 'cat',\n",
       " 'called',\n",
       " 'facebook',\n",
       " 'starting',\n",
       " 'train',\n",
       " 'study',\n",
       " 'june',\n",
       " 'lucky',\n",
       " 'asleep',\n",
       " 'drive',\n",
       " 'shower',\n",
       " 'aw',\n",
       " 'walk',\n",
       " 'lmao',\n",
       " 'bring',\n",
       " 'hand',\n",
       " 'nite',\n",
       " 'move',\n",
       " 'crap',\n",
       " 'anyway',\n",
       " 'hungry',\n",
       " 'turn',\n",
       " 'heading',\n",
       " 'afternoon',\n",
       " 'instead',\n",
       " 'sleeping',\n",
       " 'xd',\n",
       " 'red',\n",
       " 'jealous',\n",
       " 'reason',\n",
       " 'enjoying',\n",
       " 'bout',\n",
       " 'test',\n",
       " 'finish',\n",
       " 'album',\n",
       " 'wonderful',\n",
       " 'meeting',\n",
       " 'page',\n",
       " 'second',\n",
       " 'sore',\n",
       " 'mad',\n",
       " 'definitely',\n",
       " 'city',\n",
       " 'yea',\n",
       " 'bye',\n",
       " 'hoping',\n",
       " 'ice',\n",
       " 'story',\n",
       " 'soooo',\n",
       " 'set',\n",
       " 'homework',\n",
       " 'running',\n",
       " 'lady',\n",
       " 'together',\n",
       " 'cut',\n",
       " 'high',\n",
       " 'bday',\n",
       " 'mother',\n",
       " 'died',\n",
       " 'congrats',\n",
       " 'couple',\n",
       " 'top',\n",
       " 'ask',\n",
       " 'laptop',\n",
       " 'dead',\n",
       " 'happened',\n",
       " 'goin',\n",
       " 'fucking',\n",
       " 'fail',\n",
       " 'was',\n",
       " 'message',\n",
       " 'sometimes',\n",
       " 'le',\n",
       " 'holiday',\n",
       " 'dear',\n",
       " 'write',\n",
       " 'moment',\n",
       " 'won',\n",
       " 'store',\n",
       " 'min',\n",
       " 'nd',\n",
       " 'nap',\n",
       " 'sigh',\n",
       " 'visit',\n",
       " 'tour',\n",
       " 'fall',\n",
       " 'evening',\n",
       " 'church',\n",
       " 'ipod',\n",
       " 'doesnt',\n",
       " 'water',\n",
       " 'worry',\n",
       " 'short',\n",
       " 'ive',\n",
       " 'youtube',\n",
       " 'side',\n",
       " 'ppl',\n",
       " 'star',\n",
       " 'tea',\n",
       " 'close',\n",
       " 'foot',\n",
       " 'town',\n",
       " 'smile',\n",
       " 'happen',\n",
       " 'dance',\n",
       " 'perfect',\n",
       " 'point',\n",
       " 'lil',\n",
       " 'havent',\n",
       " '[',\n",
       " 'mood',\n",
       " 'listen',\n",
       " 'studying',\n",
       " 'weird',\n",
       " 'seem',\n",
       " 'ride',\n",
       " 'hang',\n",
       " 'list',\n",
       " 'gym',\n",
       " 'loving',\n",
       " 'gt',\n",
       " 'date',\n",
       " 'catch',\n",
       " 'account',\n",
       " 'english',\n",
       " 'fb',\n",
       " 'wonder',\n",
       " 'cream',\n",
       " 'ate',\n",
       " 'knew',\n",
       " 'interesting',\n",
       " 'episode',\n",
       " 'chocolate',\n",
       " 'clean',\n",
       " 'agree',\n",
       " 'writing',\n",
       " 'worst',\n",
       " 'c',\n",
       " 'mum',\n",
       " 'pool',\n",
       " 'm',\n",
       " 'saying',\n",
       " 'band',\n",
       " 'ahh',\n",
       " 'e',\n",
       " 'worth',\n",
       " 'supposed',\n",
       " 'da',\n",
       " 'wishing',\n",
       " 'london',\n",
       " 'chance',\n",
       " 'air',\n",
       " 'broken',\n",
       " 'wedding',\n",
       " 'fast',\n",
       " 'vote',\n",
       " 'flight',\n",
       " 'via',\n",
       " 'throat',\n",
       " 'unfortunately',\n",
       " 'xxx',\n",
       " 'line',\n",
       " 'past',\n",
       " 'driving',\n",
       " 'pick',\n",
       " 'three',\n",
       " 'moving',\n",
       " 'park',\n",
       " 'team',\n",
       " 'sent',\n",
       " 'forget',\n",
       " 'math',\n",
       " 'gave',\n",
       " 'yep',\n",
       " 'question',\n",
       " 'sunshine',\n",
       " 'black',\n",
       " 'understand',\n",
       " 'cleaning',\n",
       " 'followfriday',\n",
       " 'horrible',\n",
       " 'sleepy',\n",
       " 'card',\n",
       " 'number',\n",
       " 'drinking',\n",
       " 'answer',\n",
       " 'college',\n",
       " 'jonas',\n",
       " 'tweeting',\n",
       " 'bet',\n",
       " 'website',\n",
       " 'sat',\n",
       " 'comment',\n",
       " 'k',\n",
       " 'mac',\n",
       " 'upset',\n",
       " 'worse',\n",
       " 'rather',\n",
       " 'em',\n",
       " 'green',\n",
       " 'paper',\n",
       " 'fell',\n",
       " 'project',\n",
       " 'under',\n",
       " 'longer',\n",
       " 'slow',\n",
       " 'moon',\n",
       " 'beer',\n",
       " 'parent',\n",
       " 'hmm',\n",
       " 'award',\n",
       " 'cake',\n",
       " 'scared',\n",
       " 'special',\n",
       " 'easy',\n",
       " 'due',\n",
       " 'apparently',\n",
       " 'bus',\n",
       " 't',\n",
       " 'v',\n",
       " 'spent',\n",
       " 'leg',\n",
       " 'tuesday',\n",
       " 'huge',\n",
       " 'body',\n",
       " 'vacation',\n",
       " 'shop',\n",
       " 'wear',\n",
       " 'y',\n",
       " 'hr',\n",
       " 'mtv',\n",
       " 'spend',\n",
       " 'mr',\n",
       " 'dress',\n",
       " 'isnt',\n",
       " 'hanging',\n",
       " 'plus',\n",
       " 'join',\n",
       " 'fair',\n",
       " 'flu',\n",
       " 'beat',\n",
       " 'nope',\n",
       " 'earlier',\n",
       " 'thx',\n",
       " 'figure',\n",
       " 'finger',\n",
       " 'blue',\n",
       " 'shoe',\n",
       " 'wondering',\n",
       " 'wtf',\n",
       " 'cousin',\n",
       " 'voice',\n",
       " 'kill',\n",
       " 'sims',\n",
       " 'during',\n",
       " 'shame',\n",
       " 'lazy',\n",
       " 'miley',\n",
       " 'support',\n",
       " 'white',\n",
       " 'uk',\n",
       " 'slept',\n",
       " 'camera',\n",
       " 'forever',\n",
       " 'co',\n",
       " 'chat',\n",
       " 'stomach',\n",
       " 'babe',\n",
       " 'thursday',\n",
       " 'shot',\n",
       " 'load',\n",
       " 'ahhh',\n",
       " 'boyfriend',\n",
       " 'warm',\n",
       " 'window',\n",
       " 'looked',\n",
       " 'son',\n",
       " 'learn',\n",
       " 'july',\n",
       " 'sadly',\n",
       " 'inside',\n",
       " 'especially',\n",
       " 'idk',\n",
       " 'power',\n",
       " 'hes',\n",
       " 'different',\n",
       " 'woman',\n",
       " 'fix',\n",
       " 'die',\n",
       " 'meant',\n",
       " 'sign',\n",
       " 'us',\n",
       " 'age',\n",
       " 'bike',\n",
       " 'quot',\n",
       " 'david',\n",
       " 'light',\n",
       " 'airport',\n",
       " 'met',\n",
       " 'garden',\n",
       " 'google',\n",
       " 'liked',\n",
       " 'pizza',\n",
       " 'case',\n",
       " 'tom',\n",
       " 'myspace',\n",
       " 'officially',\n",
       " 'laugh',\n",
       " 'road',\n",
       " 'safe',\n",
       " 'apple',\n",
       " 'rainy',\n",
       " 'save',\n",
       " 'yr',\n",
       " 'tummy',\n",
       " 'small',\n",
       " 'absolutely',\n",
       " 'shall',\n",
       " 'bbq',\n",
       " 'graduation',\n",
       " 'service',\n",
       " 'shirt',\n",
       " 'yummy',\n",
       " 'cd',\n",
       " 'rip',\n",
       " 'chicken',\n",
       " 'worked',\n",
       " 'felt',\n",
       " 'decided',\n",
       " 'luv',\n",
       " 'radio',\n",
       " 'share',\n",
       " 'proud',\n",
       " 'film',\n",
       " 'hospital',\n",
       " 'box',\n",
       " 'each',\n",
       " 'si',\n",
       " 'except',\n",
       " 'yall',\n",
       " 'wit',\n",
       " 'needed',\n",
       " 'cup',\n",
       " 'g',\n",
       " 'note',\n",
       " 'fact',\n",
       " 'yourself',\n",
       " 'order',\n",
       " 'played',\n",
       " 'club',\n",
       " 'goodbye',\n",
       " 'has',\n",
       " 'hubby',\n",
       " 'gorgeous',\n",
       " 'lonely',\n",
       " 'annoying',\n",
       " 'bitch',\n",
       " 'exactly',\n",
       " 'wine',\n",
       " 'dm',\n",
       " 'alright',\n",
       " 'lets',\n",
       " 'doctor',\n",
       " 'exciting',\n",
       " 'front',\n",
       " 'yup',\n",
       " 'looks',\n",
       " 'fly',\n",
       " 'twilight',\n",
       " 'issue',\n",
       " 'wednesday',\n",
       " 'interview',\n",
       " 'taken',\n",
       " 'french',\n",
       " 'yo',\n",
       " 'glass',\n",
       " 'company',\n",
       " 'living',\n",
       " 'lame',\n",
       " 'mothers',\n",
       " 'xoxo',\n",
       " '`',\n",
       " 'near',\n",
       " 'packing',\n",
       " 'turned',\n",
       " 'woo',\n",
       " 'business',\n",
       " 'bag',\n",
       " 'realized',\n",
       " 'scary',\n",
       " 'happens',\n",
       " 'behind',\n",
       " 'bc',\n",
       " 'smell',\n",
       " 're',\n",
       " 'couldnt',\n",
       " 'bar',\n",
       " 'ouch',\n",
       " 'sold',\n",
       " 'version',\n",
       " 'download',\n",
       " 'jus',\n",
       " 'waking',\n",
       " 'gettin',\n",
       " 'yours',\n",
       " 'storm',\n",
       " 'drunk',\n",
       " 'touch',\n",
       " 'guitar',\n",
       " 'killing',\n",
       " 'wife',\n",
       " 'giving',\n",
       " 'sounds',\n",
       " 'round',\n",
       " 'mate',\n",
       " 'lose',\n",
       " 'daughter',\n",
       " 'walking',\n",
       " 'aint',\n",
       " 'clothes',\n",
       " 'ear',\n",
       " 'matter',\n",
       " 'door',\n",
       " 'deal',\n",
       " 'although',\n",
       " 'app',\n",
       " 'puppy',\n",
       " 'along',\n",
       " 'enjoyed',\n",
       " 'l',\n",
       " 'cheer',\n",
       " 'relaxing',\n",
       " 'event',\n",
       " 'single',\n",
       " 'hold',\n",
       " 'revision',\n",
       " 'hahah',\n",
       " 'pas',\n",
       " 'ps',\n",
       " 'terrible',\n",
       " 'bro',\n",
       " 'everybody',\n",
       " 'plane',\n",
       " 'alot',\n",
       " 'mile',\n",
       " 'passed',\n",
       " 'bb',\n",
       " 'posted',\n",
       " 'fantastic',\n",
       " 'random',\n",
       " 'whatever',\n",
       " 'sit',\n",
       " 'country',\n",
       " 'mommy',\n",
       " 'staying',\n",
       " 'asked',\n",
       " 'taste',\n",
       " 'dvd',\n",
       " 'ahead',\n",
       " 'hangover',\n",
       " 'shoot',\n",
       " 'hotel',\n",
       " 'group',\n",
       " 'sale',\n",
       " 'arm',\n",
       " 'indeed',\n",
       " 'ball',\n",
       " ...]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv.index2word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539e8d4f",
   "metadata": {},
   "source": [
    "**Load word2vec model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae26f65d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T13:31:10.149498Z",
     "start_time": "2021-06-10T13:31:08.971508Z"
    }
   },
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec.load(\"word2vec.model\")\n",
    "review = df.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244e40e3",
   "metadata": {},
   "source": [
    "# Chuyển đổi text sang vector "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ef44b1",
   "metadata": {},
   "source": [
    "**Tính trung bình giá trị vector của từng câu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8e1fb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T02:59:21.975119Z",
     "start_time": "2021-05-19T02:59:21.962090Z"
    }
   },
   "outputs": [],
   "source": [
    "def featureVecMethod(words, model, num_features=100):\n",
    "    # Pre-initialising empty numpy array for speed\n",
    "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    \n",
    "    #Converting Index2Word which is a list to a set for better speed in the execution.\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    for word in  words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec,model.wv.__getitem__(word))\n",
    "    \n",
    "    # Dividing the result by number of words to get average\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f398ea8",
   "metadata": {},
   "source": [
    "**Hàm trả về mảng 2 chiều chứa giá trị vector của tất caả các caâu trong dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e2d85e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T03:00:35.062122Z",
     "start_time": "2021-05-19T03:00:35.042142Z"
    }
   },
   "outputs": [],
   "source": [
    "def getAvgFeatureVecs(reviews, model, num_features=100):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        # Printing a status message every 1000th review\n",
    "#         if counter%10000 == 0:\n",
    "        print(\"Review %d of %d\"%(counter,len(reviews)))\n",
    "            \n",
    "        reviewFeatureVecs[counter] = featureVecMethod(review, model, num_features)\n",
    "        counter = counter+1\n",
    "        \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5ccf63",
   "metadata": {},
   "source": [
    "**Vì tính mảng 2 chiều trên hơi laâu nên khi tính xong ta sẽ lưu vào file tên là avarage vector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be293756",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-05-19T03:00:36.263Z"
    }
   },
   "outputs": [],
   "source": [
    "trainDataVecs = getAvgFeatureVecs(token_sent, model, 100)\n",
    "np.save('/pytorch dataset and model/avarage vector.npy', trainDataVecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f2ff92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T13:50:14.856678Z",
     "start_time": "2021-06-10T13:50:14.836687Z"
    }
   },
   "source": [
    "**Load xem file có dđúng không và lưu vào biến X để chuẩn bị train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51c320db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T09:58:00.797369Z",
     "start_time": "2021-05-19T09:57:56.659351Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1596821, 100)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.load('/pytorch dataset and model/avarage vector.npy')\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baf6c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.target\n",
    "y= np.array(y)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f723fb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 0\n",
    "nan_arr = []\n",
    "for ind, value in enumerate(X):\n",
    "    if np.isnan(value).any():\n",
    "        nan_arr.append(ind)\n",
    "#         print(ind)\n",
    "newX = np.delete(X, nan_arr, axis=0)\n",
    "newY = np.delete(y, nan_arr)\n",
    "newX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ae361b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d8c5f82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T09:58:33.779385Z",
     "start_time": "2021-05-19T09:58:33.227354Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(newX, newY, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4eb061e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T08:52:09.788048Z",
     "start_time": "2021-05-19T08:52:09.768064Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1116218, 100) (1116218,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6da8efee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T08:33:08.573622Z",
     "start_time": "2021-05-19T08:32:51.591640Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   16.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(verbose=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lreg = LogisticRegression(solver='lbfgs', verbose=1) \n",
    "lreg.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2ea1b2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T13:19:51.175551Z",
     "start_time": "2021-06-10T13:19:51.158532Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "model = LogisticRegression()\n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "penalty = ['l2']\n",
    "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
    "# define grid search\n",
    "grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
    "grid_result = grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff28a57",
   "metadata": {},
   "source": [
    "**Ta thay mô hình có độ chính xác là 73%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75b4ee08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T08:33:17.353656Z",
     "start_time": "2021-05-19T08:33:15.383643Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[176616  62088]\n",
      " [ 65814 173862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.74      0.73    238704\n",
      "           4       0.74      0.73      0.73    239676\n",
      "\n",
      "    accuracy                           0.73    478380\n",
      "   macro avg       0.73      0.73      0.73    478380\n",
      "weighted avg       0.73      0.73      0.73    478380\n",
      "\n",
      "0.7326351436096826\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "y_pred = lreg.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f79799",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T09:42:09.973359Z",
     "start_time": "2021-05-19T09:42:08.259350Z"
    }
   },
   "source": [
    "**Lưu và load model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c4dd067",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T09:54:27.969351Z",
     "start_time": "2021-05-19T09:54:27.964367Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(lreg, open(\"/pytorch dataset and model/logistic_classifier\", 'wb'))\n",
    "# pickle.dump(classifier, open(\"random_forest_classifier\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59c8858b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T09:54:31.524375Z",
     "start_time": "2021-05-19T09:54:29.956370Z"
    }
   },
   "outputs": [],
   "source": [
    "logistic_model = pickle.load(open(\"/pytorch dataset and model/logistic_classifier\", 'rb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch (conda)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
